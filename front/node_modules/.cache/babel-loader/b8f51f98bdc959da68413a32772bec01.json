{"ast":null,"code":"\"use strict\";\n\nvar _interopRequireDefault = require(\"@babel/runtime/helpers/interopRequireDefault\");\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = createDirectLineSpeechAdapters;\n\nvar _botframeworkDirectlinespeechSdk = require(\"botframework-directlinespeech-sdk\");\n\nvar _createMicrophoneAudioConfigAndAudioContext = _interopRequireDefault(require(\"./speech/createMicrophoneAudioConfigAndAudioContext\"));\n\nvar DEFAULT_LANGUAGE = 'en-US'; // TODO: When using DLSpeech via bundle, we will add our own MicrophoneAudioConfig.\n\nfunction createDirectLineSpeechAdapters(_ref) {\n  var _window, _window$navigator;\n\n  var audioConfig = _ref.audioConfig,\n      audioContext = _ref.audioContext,\n      audioInputDeviceId = _ref.audioInputDeviceId,\n      enableInternalHTTPSupport = _ref.enableInternalHTTPSupport,\n      enableTelemetry = _ref.enableTelemetry,\n      fetchCredentials = _ref.fetchCredentials,\n      speechRecognitionEndpointId = _ref.speechRecognitionEndpointId,\n      _ref$speechRecognitio = _ref.speechRecognitionLanguage,\n      speechRecognitionLanguage = _ref$speechRecognitio === void 0 ? ((_window = window) === null || _window === void 0 ? void 0 : (_window$navigator = _window.navigator) === null || _window$navigator === void 0 ? void 0 : _window$navigator.language) || DEFAULT_LANGUAGE : _ref$speechRecognitio,\n      speechSynthesisDeploymentId = _ref.speechSynthesisDeploymentId,\n      speechSynthesisOutputFormat = _ref.speechSynthesisOutputFormat,\n      textNormalization = _ref.textNormalization,\n      userID = _ref.userID,\n      username = _ref.username;\n\n  if (audioConfig) {\n    audioInputDeviceId && console.warn('botframework-webchat: \"audioConfig\" and \"audioInputDeviceId\" cannot be set at the same time; ignoring \"audioInputDeviceId\".');\n    audioContext && console.warn('botframework-webchat: \"audioConfig\" and \"audioContext\" cannot be set at the same time; ignoring \"audioContext\" for speech recognition.');\n  } else if (!window.navigator.mediaDevices) {\n    // If the browser does not support or allow microphone access, we will continue to create Direct Line Speech adapter without custom \"audioConfig\" and \"audioContext\".\n    // In Direct Line Speech SDK, it will disable speech functionality, only leaving text chat available via the protocol.\n    console.warn('botframework-webchat: Your browser does not support or allow microphone access or the page is not loaded via HTTPS or localhost. Speech is disabled for Direct Line Speech. However, you may pass a custom \"audioConfig\" to enable speech in this environment.');\n  } else {\n    var _createMicrophoneAudi = (0, _createMicrophoneAudioConfigAndAudioContext.default)({\n      audioContext: audioContext,\n      audioInputDeviceId: audioInputDeviceId,\n      enableTelemetry: enableTelemetry\n    });\n\n    audioConfig = _createMicrophoneAudi.audioConfig;\n    audioContext = _createMicrophoneAudi.audioContext;\n  }\n\n  return (0, _botframeworkDirectlinespeechSdk.createAdapters)({\n    audioConfig: audioConfig,\n    audioContext: audioContext,\n    enableInternalHTTPSupport: enableInternalHTTPSupport,\n    enableTelemetry: enableTelemetry,\n    fetchCredentials: fetchCredentials,\n    speechRecognitionEndpointId: speechRecognitionEndpointId,\n    speechRecognitionLanguage: speechRecognitionLanguage,\n    speechSynthesisDeploymentId: speechSynthesisDeploymentId,\n    speechSynthesisOutputFormat: speechSynthesisOutputFormat,\n    textNormalization: textNormalization,\n    userID: userID,\n    username: username\n  });\n}","map":{"version":3,"mappings":";;;;;;;;;AACA;;AAOA;;AAEA,IAAMA,gBAAgB,GAAG,OAAzB,C,CAEA;;AACe,SAASC,8BAAT,OA+Bb;AAAA;;AAAA,MA9BAC,WA8BA,QA9BAA,WA8BA;AAAA,MA7BAC,YA6BA,QA7BAA,YA6BA;AAAA,MA5BAC,kBA4BA,QA5BAA,kBA4BA;AAAA,MA3BAC,yBA2BA,QA3BAA,yBA2BA;AAAA,MA1BAC,eA0BA,QA1BAA,eA0BA;AAAA,MAzBAC,gBAyBA,QAzBAA,gBAyBA;AAAA,MAxBAC,2BAwBA,QAxBAA,2BAwBA;AAAA,mCAvBAC,yBAuBA;AAAA,MAvBAA,yBAuBA,sCAvB4B,wBAAM,IAAN,+DAAQC,SAAR,wEAAmBC,QAAnB,KAA+BX,gBAuB3D;AAAA,MAtBAY,2BAsBA,QAtBAA,2BAsBA;AAAA,MArBAC,2BAqBA,QArBAA,2BAqBA;AAAA,MApBAC,iBAoBA,QApBAA,iBAoBA;AAAA,MAnBAC,MAmBA,QAnBAA,MAmBA;AAAA,MAlBAC,QAkBA,QAlBAA,QAkBA;;AACA,MAAId,WAAJ,EAAiB;AACfE,sBAAkB,IAChBa,OAAO,CAACC,IAARD,CACE,6HADFA,CADFb;AAKAD,gBAAY,IACVc,OAAO,CAACC,IAARD,CACE,wIADFA,CADFd;AANF,SAUO,IAAI,CAACgB,MAAM,CAACT,SAAPS,CAAiBC,YAAtB,EAAoC;AACzC;AACA;AACAH,WAAO,CAACC,IAARD,CACE,gQADFA;AAHK,SAMA;AAAA,gCAC4B,yDAA2C;AAC1Ed,kBAAY,EAAZA,YAD0E;AAE1EC,wBAAkB,EAAlBA,kBAF0E;AAG1EE,qBAAe,EAAfA;AAH0E,KAA3C,CAD5B;;AACFJ,eADE,yBACFA;AAAaC,gBADX,yBACWA;AAKjB;;AAED,SAAO,qDAAe;AACpBD,eAAW,EAAXA,WADoB;AAEpBC,gBAAY,EAAZA,YAFoB;AAGpBE,6BAAyB,EAAzBA,yBAHoB;AAIpBC,mBAAe,EAAfA,eAJoB;AAKpBC,oBAAgB,EAAhBA,gBALoB;AAMpBC,+BAA2B,EAA3BA,2BANoB;AAOpBC,6BAAyB,EAAzBA,yBAPoB;AAQpBG,+BAA2B,EAA3BA,2BARoB;AASpBC,+BAA2B,EAA3BA,2BAToB;AAUpBC,qBAAiB,EAAjBA,iBAVoB;AAWpBC,UAAM,EAANA,MAXoB;AAYpBC,YAAQ,EAARA;AAZoB,GAAf,CAAP;AAcD","names":["DEFAULT_LANGUAGE","createDirectLineSpeechAdapters","audioConfig","audioContext","audioInputDeviceId","enableInternalHTTPSupport","enableTelemetry","fetchCredentials","speechRecognitionEndpointId","speechRecognitionLanguage","navigator","language","speechSynthesisDeploymentId","speechSynthesisOutputFormat","textNormalization","userID","username","console","warn","window","mediaDevices"],"sources":["/Users/dylanmurray/Sweng-2022/front/node_modules/botframework-webchat/lib/src/createDirectLineSpeechAdapters.ts"],"sourcesContent":["import { AudioConfig } from 'microsoft-cognitiveservices-speech-sdk';\nimport { createAdapters } from 'botframework-directlinespeech-sdk';\nimport { DirectLineJSBotConnection } from 'botframework-webchat-core';\nimport { WebSpeechPonyfill } from 'botframework-webchat-api';\n\nimport CognitiveServicesAudioOutputFormat from './types/CognitiveServicesAudioOutputFormat';\nimport CognitiveServicesCredentials from './types/CognitiveServicesCredentials';\nimport CognitiveServicesTextNormalization from './types/CognitiveServicesTextNormalization';\nimport createMicrophoneAudioConfigAndAudioContext from './speech/createMicrophoneAudioConfigAndAudioContext';\n\nconst DEFAULT_LANGUAGE = 'en-US';\n\n// TODO: When using DLSpeech via bundle, we will add our own MicrophoneAudioConfig.\nexport default function createDirectLineSpeechAdapters({\n  audioConfig,\n  audioContext,\n  audioInputDeviceId,\n  enableInternalHTTPSupport,\n  enableTelemetry,\n  fetchCredentials,\n  speechRecognitionEndpointId,\n  speechRecognitionLanguage = window?.navigator?.language || DEFAULT_LANGUAGE,\n  speechSynthesisDeploymentId,\n  speechSynthesisOutputFormat,\n  textNormalization,\n  userID,\n  username\n}: {\n  audioConfig?: AudioConfig;\n  audioContext?: AudioContext;\n  audioInputDeviceId?: string;\n  enableInternalHTTPSupport?: true;\n  enableTelemetry?: true;\n  fetchCredentials: CognitiveServicesCredentials;\n  speechRecognitionEndpointId?: string;\n  speechRecognitionLanguage?: string;\n  speechSynthesisDeploymentId?: string;\n  speechSynthesisOutputFormat?: CognitiveServicesAudioOutputFormat;\n  textNormalization?: CognitiveServicesTextNormalization;\n  userID?: string;\n  username?: string;\n}): {\n  directLine: DirectLineJSBotConnection;\n  webSpeechPonyfill: WebSpeechPonyfill;\n} {\n  if (audioConfig) {\n    audioInputDeviceId &&\n      console.warn(\n        'botframework-webchat: \"audioConfig\" and \"audioInputDeviceId\" cannot be set at the same time; ignoring \"audioInputDeviceId\".'\n      );\n\n    audioContext &&\n      console.warn(\n        'botframework-webchat: \"audioConfig\" and \"audioContext\" cannot be set at the same time; ignoring \"audioContext\" for speech recognition.'\n      );\n  } else if (!window.navigator.mediaDevices) {\n    // If the browser does not support or allow microphone access, we will continue to create Direct Line Speech adapter without custom \"audioConfig\" and \"audioContext\".\n    // In Direct Line Speech SDK, it will disable speech functionality, only leaving text chat available via the protocol.\n    console.warn(\n      'botframework-webchat: Your browser does not support or allow microphone access or the page is not loaded via HTTPS or localhost. Speech is disabled for Direct Line Speech. However, you may pass a custom \"audioConfig\" to enable speech in this environment.'\n    );\n  } else {\n    ({ audioConfig, audioContext } = createMicrophoneAudioConfigAndAudioContext({\n      audioContext,\n      audioInputDeviceId,\n      enableTelemetry\n    }));\n  }\n\n  return createAdapters({\n    audioConfig,\n    audioContext,\n    enableInternalHTTPSupport,\n    enableTelemetry,\n    fetchCredentials,\n    speechRecognitionEndpointId,\n    speechRecognitionLanguage,\n    speechSynthesisDeploymentId,\n    speechSynthesisOutputFormat,\n    textNormalization,\n    userID,\n    username\n  });\n}\n"]},"metadata":{},"sourceType":"script"}