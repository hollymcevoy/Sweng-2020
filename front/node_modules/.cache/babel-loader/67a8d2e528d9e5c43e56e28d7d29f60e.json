{"ast":null,"code":"\"use strict\";\n\nvar _interopRequireDefault = require(\"@babel/runtime/helpers/interopRequireDefault\");\n\nvar _typeof = require(\"@babel/runtime/helpers/typeof\");\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = _default;\n\nvar _regenerator = _interopRequireDefault(require(\"@babel/runtime/regenerator\"));\n\nvar _asyncToGenerator2 = _interopRequireDefault(require(\"@babel/runtime/helpers/asyncToGenerator\"));\n\nvar _classCallCheck2 = _interopRequireDefault(require(\"@babel/runtime/helpers/classCallCheck\"));\n\nvar _createClass2 = _interopRequireDefault(require(\"@babel/runtime/helpers/createClass\"));\n\nvar _inherits2 = _interopRequireDefault(require(\"@babel/runtime/helpers/inherits\"));\n\nvar _possibleConstructorReturn2 = _interopRequireDefault(require(\"@babel/runtime/helpers/possibleConstructorReturn\"));\n\nvar _getPrototypeOf2 = _interopRequireDefault(require(\"@babel/runtime/helpers/getPrototypeOf\"));\n\nvar _abortControllerEs = require(\"abort-controller-es5\");\n\nvar _SpeechToText = require(\"web-speech-cognitive-services/lib/SpeechServices/SpeechToText\");\n\nvar _createTaskQueue2 = _interopRequireDefault(require(\"./createTaskQueue\"));\n\nvar _es = _interopRequireWildcard(require(\"event-target-shim/es5\"));\n\nvar _playCognitiveServicesStream = _interopRequireDefault(require(\"./playCognitiveServicesStream\"));\n\nvar _playWhiteNoise = _interopRequireDefault(require(\"./playWhiteNoise\"));\n\nvar _SpeechSynthesisAudioStreamUtterance = _interopRequireDefault(require(\"./SpeechSynthesisAudioStreamUtterance\"));\n\nfunction _getRequireWildcardCache(nodeInterop) {\n  if (typeof WeakMap !== \"function\") return null;\n  var cacheBabelInterop = new WeakMap();\n  var cacheNodeInterop = new WeakMap();\n  return (_getRequireWildcardCache = function _getRequireWildcardCache(nodeInterop) {\n    return nodeInterop ? cacheNodeInterop : cacheBabelInterop;\n  })(nodeInterop);\n}\n\nfunction _interopRequireWildcard(obj, nodeInterop) {\n  if (!nodeInterop && obj && obj.__esModule) {\n    return obj;\n  }\n\n  if (obj === null || _typeof(obj) !== \"object\" && typeof obj !== \"function\") {\n    return {\n      default: obj\n    };\n  }\n\n  var cache = _getRequireWildcardCache(nodeInterop);\n\n  if (cache && cache.has(obj)) {\n    return cache.get(obj);\n  }\n\n  var newObj = {};\n  var hasPropertyDescriptor = Object.defineProperty && Object.getOwnPropertyDescriptor;\n\n  for (var key in obj) {\n    if (key !== \"default\" && Object.prototype.hasOwnProperty.call(obj, key)) {\n      var desc = hasPropertyDescriptor ? Object.getOwnPropertyDescriptor(obj, key) : null;\n\n      if (desc && (desc.get || desc.set)) {\n        Object.defineProperty(newObj, key, desc);\n      } else {\n        newObj[key] = obj[key];\n      }\n    }\n  }\n\n  newObj.default = obj;\n\n  if (cache) {\n    cache.set(obj, newObj);\n  }\n\n  return newObj;\n}\n\nfunction _createSuper(Derived) {\n  var hasNativeReflectConstruct = _isNativeReflectConstruct();\n\n  return function _createSuperInternal() {\n    var Super = (0, _getPrototypeOf2.default)(Derived),\n        result;\n\n    if (hasNativeReflectConstruct) {\n      var NewTarget = (0, _getPrototypeOf2.default)(this).constructor;\n      result = Reflect.construct(Super, arguments, NewTarget);\n    } else {\n      result = Super.apply(this, arguments);\n    }\n\n    return (0, _possibleConstructorReturn2.default)(this, result);\n  };\n}\n\nfunction _isNativeReflectConstruct() {\n  if (typeof Reflect === \"undefined\" || !Reflect.construct) return false;\n  if (Reflect.construct.sham) return false;\n  if (typeof Proxy === \"function\") return true;\n\n  try {\n    Boolean.prototype.valueOf.call(Reflect.construct(Boolean, [], function () {}));\n    return true;\n  } catch (e) {\n    return false;\n  }\n}\n\nfunction _default(_ref) {\n  var audioContext = _ref.audioContext,\n      enableTelemetry = _ref.enableTelemetry,\n      _ref$ponyfill = _ref.ponyfill,\n      ponyfill = _ref$ponyfill === void 0 ? {\n    AudioContext: window.AudioContext || window.webkitAudioContext\n  } : _ref$ponyfill,\n      recognizer = _ref.recognizer,\n      textNormalization = _ref.textNormalization;\n\n  if (!ponyfill.AudioContext) {\n    console.warn('botframework-directlinespeech-sdk: This browser does not support Web Audio API. Speech support is disabled.');\n    return function () {\n      return {};\n    };\n  }\n\n  return function () {\n    var _createSpeechRecognit = (0, _SpeechToText.createSpeechRecognitionPonyfillFromRecognizer)({\n      createRecognizer: function createRecognizer() {\n        return recognizer;\n      },\n      enableTelemetry: enableTelemetry,\n      looseEvents: true,\n      textNormalization: textNormalization\n    }),\n        SpeechGrammarList = _createSpeechRecognit.SpeechGrammarList,\n        SpeechRecognition = _createSpeechRecognit.SpeechRecognition;\n\n    if (!audioContext) {\n      audioContext = new ponyfill.AudioContext();\n    }\n\n    var _createTaskQueue = (0, _createTaskQueue2.default)(),\n        cancelAll = _createTaskQueue.cancelAll,\n        push = _createTaskQueue.push;\n\n    var SpeechSynthesis = /*#__PURE__*/function (_EventTarget) {\n      (0, _inherits2.default)(SpeechSynthesis, _EventTarget);\n\n      var _super = _createSuper(SpeechSynthesis);\n\n      function SpeechSynthesis() {\n        (0, _classCallCheck2.default)(this, SpeechSynthesis);\n        return _super.apply(this, arguments);\n      }\n\n      (0, _createClass2.default)(SpeechSynthesis, [{\n        key: \"cancel\",\n        value: function cancel() {\n          cancelAll();\n        } // Returns an empty array.\n        // Synthesis is done on the bot side, the content of the voice list is not meaningful on the client side.\n\n      }, {\n        key: \"getVoices\",\n        value: function getVoices() {\n          return [];\n        }\n      }, {\n        key: \"speak\",\n        value: function speak(utterance) {\n          var _push = push(function () {\n            var controller = new _abortControllerEs.AbortController();\n            var signal = controller.signal;\n            return {\n              abort: controller.abort.bind(controller),\n              result: (0, _asyncToGenerator2.default)( /*#__PURE__*/_regenerator.default.mark(function _callee() {\n                return _regenerator.default.wrap(function _callee$(_context) {\n                  while (1) {\n                    switch (_context.prev = _context.next) {\n                      case 0:\n                        utterance.dispatchEvent(new _es.Event('start'));\n                        _context.prev = 1;\n\n                        if (!utterance.audioStream) {\n                          _context.next = 7;\n                          break;\n                        }\n\n                        _context.next = 5;\n                        return (0, _playCognitiveServicesStream.default)(audioContext, utterance.audioStream, {\n                          signal: signal\n                        });\n\n                      case 5:\n                        _context.next = 9;\n                        break;\n\n                      case 7:\n                        _context.next = 9;\n                        return (0, _playWhiteNoise.default)(audioContext);\n\n                      case 9:\n                        _context.next = 15;\n                        break;\n\n                      case 11:\n                        _context.prev = 11;\n                        _context.t0 = _context[\"catch\"](1);\n\n                        if (!(_context.t0.message !== 'aborted')) {\n                          _context.next = 15;\n                          break;\n                        }\n\n                        return _context.abrupt(\"return\", utterance.dispatchEvent(new ErrorEvent(_context.t0)));\n\n                      case 15:\n                        utterance.dispatchEvent(new _es.Event('end'));\n\n                      case 16:\n                      case \"end\":\n                        return _context.stop();\n                    }\n                  }\n                }, _callee, null, [[1, 11]]);\n              }))()\n            };\n          }),\n              result = _push.result; // Catching the error to prevent uncaught promise error due to cancellation.\n\n\n          result.catch(function (error) {\n            if (!/^cancelled/i.test(error.message)) {\n              throw error;\n            }\n          });\n        }\n      }, {\n        key: \"onvoiceschanged\",\n        get: function get() {\n          return (0, _es.getEventAttributeValue)(this, 'voiceschanged');\n        },\n        set: function set(value) {\n          (0, _es.setEventAttributeValue)(this, 'voiceschanged', value);\n        }\n      }]);\n      return SpeechSynthesis;\n    }(_es.default);\n\n    return {\n      SpeechGrammarList: SpeechGrammarList,\n      SpeechRecognition: SpeechRecognition,\n      speechSynthesis: new SpeechSynthesis(),\n      SpeechSynthesisUtterance: _SpeechSynthesisAudioStreamUtterance.default\n    };\n  };\n}","map":{"version":3,"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;AAEA;;AACA;;AAEA;;AACA;;AACA;;AACA;;AACA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAEe,wBAQZ;AAAA,MAPDA,YAOC,QAPDA,YAOC;AAAA,MANDC,eAMC,QANDA,eAMC;AAAA,2BALDC,QAKC;AAAA,MALDA,QAKC,8BALU;AACTC,gBAAY,EAAEC,MAAM,CAACD,YAAPC,IAAuBA,MAAM,CAACC;AADnC,GAKV;AAAA,MAFDC,UAEC,QAFDA,UAEC;AAAA,MADDC,iBACC,QADDA,iBACC;;AACD,MAAI,CAACL,QAAQ,CAACC,YAAd,EAA4B;AAC1BK,WAAO,CAACC,IAARD,CACE,6GADFA;AAIA,WAAO;AAAA,aAAO,EAAP;AAAP;AACD;;AAED,SAAO,YAAM;AACX,gCAAiD,iEAA8C;AAC7FE,sBAAgB,EAAE;AAAA,eAAMJ,UAAN;AAD2E;AAE7FL,qBAAe,EAAfA,eAF6F;AAG7FU,iBAAW,EAAE,IAHgF;AAI7FJ,uBAAiB,EAAjBA;AAJ6F,KAA9C,CAAjD;AAAA,QAAQK,iBAAR,yBAAQA,iBAAR;AAAA,QAA2BC,iBAA3B,yBAA2BA,iBAA3B;;AAOA,QAAI,CAACb,YAAL,EAAmB;AACjBA,kBAAY,GAAG,IAAIE,QAAQ,CAACC,YAAb,EAAfH;AACD;;AAED,2BAA4B,gCAA5B;AAAA,QAAQc,SAAR,oBAAQA,SAAR;AAAA,QAAmBC,IAAnB,oBAAmBA,IAAnB;;AAZW,QAcLC,eAdK;AAAA;;AAAA;;AAAA;AAAA;AAAA;AAAA;;AAAA;AAAAC;AAAAC,eAeT,kBAAS;AACPJ,mBAAS;AAhBF,UAmBT;AACA;;AApBS;AAAAG;AAAAC,eAqBT,qBAAY;AACV,iBAAO,EAAP;AACD;AAvBQ;AAAAD;AAAAC,eAyBT,eAAMC,SAAN,EAAiB;AACf,sBAAmBJ,IAAI,CAAC,YAAM;AAC5B,gBAAMK,UAAU,GAAG,IAAIC,kCAAJ,EAAnB;AACA,gBAAQC,MAAR,GAAmBF,UAAnB,CAAQE,MAAR;AAEA,mBAAO;AACLC,mBAAK,EAAEH,UAAU,CAACG,KAAXH,CAAiBI,IAAjBJ,CAAsBA,UAAtBA,CADF;AAELK,oBAAM,EAAE,wEAAC;AAAA;AAAA;AAAA;AAAA;AACPN,iCAAS,CAACO,aAAVP,CAAwB,IAAIQ,SAAJ,CAAU,OAAV,CAAxBR;AADOS;;AAAA,6BAIDT,SAAS,CAACU,WAJT;AAAAD;AAAA;AAAA;;AAAAA;AAAA,+BAKG,0CAA4B5B,YAA5B,EAA0CmB,SAAS,CAACU,WAApD,EAAiE;AAAEP,gCAAM,EAANA;AAAF,yBAAjE,CALH;;AAAA;AAAAM;AAAA;;AAAA;AAAAA;AAAA,+BAOG,6BAAe5B,YAAf,CAPH;;AAAA;AAAA4B;AAAA;;AAAA;AAAAA;AAAAA;;AAAA,8BAWDA,YAAME,OAAN,KAAkB,SAXjB;AAAAF;AAAA;AAAA;;AAAA,yDAYIT,SAAS,CAACO,aAAVP,CAAwB,IAAIY,UAAJ,aAAxBZ,CAZJ;;AAAA;AAgBPA,iCAAS,CAACO,aAAVP,CAAwB,IAAIQ,SAAJ,CAAU,KAAV,CAAxBR;;AAhBO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAD;AAFH,aAAP;AAJqB,YAAvB;AAAA,cAAQM,MAAR,SAAQA,MAAR,CADe,CA4Bf;;;AACAA,gBAAM,CAACO,KAAPP,CAAa,iBAAS;AACpB,gBAAI,CAAC,cAAeQ,IAAf,CAAoBC,KAAK,CAACJ,OAA1B,CAAL,EAAyC;AACvC,oBAAMI,KAAN;AACD;AAHH;AAKD;AA3DQ;AAAAjB;AAAAkB,aA6DT,eAAsB;AACpB,iBAAO,gCAAuB,IAAvB,EAA6B,eAA7B,CAAP;AA9DO;AAAAC,aAiET,aAAoBlB,KAApB,EAA2B;AACzB,0CAAuB,IAAvB,EAA6B,eAA7B,EAA8CA,KAA9C;AACD;AAnEQ;AAAA;AAAA,MAcmBmB,WAdnB;;AAsEX,WAAO;AACLzB,uBAAiB,EAAjBA,iBADK;AAELC,uBAAiB,EAAjBA,iBAFK;AAGLyB,qBAAe,EAAE,IAAItB,eAAJ,EAHZ;AAILuB,8BAAwB,EAAEC;AAJrB,KAAP;AAtEF;AA6ED","names":["audioContext","enableTelemetry","ponyfill","AudioContext","window","webkitAudioContext","recognizer","textNormalization","console","warn","createRecognizer","looseEvents","SpeechGrammarList","SpeechRecognition","cancelAll","push","SpeechSynthesis","key","value","utterance","controller","AbortController","signal","abort","bind","result","dispatchEvent","Event","_context","audioStream","message","ErrorEvent","catch","test","error","get","set","EventTarget","speechSynthesis","SpeechSynthesisUtterance","SpeechSynthesisAudioStreamUtterance"],"sources":["/Users/dylanmurray/Sweng-2022/front/node_modules/botframework-directlinespeech-sdk/lib/src/createWebSpeechPonyfillFactory.js"],"sourcesContent":["/* eslint class-methods-use-this: [\"error\", { \"exceptMethods\": [\"cancel\", \"getVoices\", \"speak\"] }] */\n\nimport { AbortController } from 'abort-controller-es5';\nimport { createSpeechRecognitionPonyfillFromRecognizer } from 'web-speech-cognitive-services/lib/SpeechServices/SpeechToText';\n\nimport createTaskQueue from './createTaskQueue';\nimport EventTarget, { Event, getEventAttributeValue, setEventAttributeValue } from 'event-target-shim/es5';\nimport playCognitiveServicesStream from './playCognitiveServicesStream';\nimport playWhiteNoise from './playWhiteNoise';\nimport SpeechSynthesisAudioStreamUtterance from './SpeechSynthesisAudioStreamUtterance';\n\nexport default function ({\n  audioContext,\n  enableTelemetry,\n  ponyfill = {\n    AudioContext: window.AudioContext || window.webkitAudioContext\n  },\n  recognizer,\n  textNormalization\n}) {\n  if (!ponyfill.AudioContext) {\n    console.warn(\n      'botframework-directlinespeech-sdk: This browser does not support Web Audio API. Speech support is disabled.'\n    );\n\n    return () => ({});\n  }\n\n  return () => {\n    const { SpeechGrammarList, SpeechRecognition } = createSpeechRecognitionPonyfillFromRecognizer({\n      createRecognizer: () => recognizer,\n      enableTelemetry,\n      looseEvents: true,\n      textNormalization\n    });\n\n    if (!audioContext) {\n      audioContext = new ponyfill.AudioContext();\n    }\n\n    const { cancelAll, push } = createTaskQueue();\n\n    class SpeechSynthesis extends EventTarget {\n      cancel() {\n        cancelAll();\n      }\n\n      // Returns an empty array.\n      // Synthesis is done on the bot side, the content of the voice list is not meaningful on the client side.\n      getVoices() {\n        return [];\n      }\n\n      speak(utterance) {\n        const { result } = push(() => {\n          const controller = new AbortController();\n          const { signal } = controller;\n\n          return {\n            abort: controller.abort.bind(controller),\n            result: (async () => {\n              utterance.dispatchEvent(new Event('start'));\n\n              try {\n                if (utterance.audioStream) {\n                  await playCognitiveServicesStream(audioContext, utterance.audioStream, { signal });\n                } else {\n                  await playWhiteNoise(audioContext);\n                }\n              } catch (error) {\n                // Either dispatch \"end\" or \"error\" event, but not both\n                if (error.message !== 'aborted') {\n                  return utterance.dispatchEvent(new ErrorEvent(error));\n                }\n              }\n\n              utterance.dispatchEvent(new Event('end'));\n            })()\n          };\n        });\n\n        // Catching the error to prevent uncaught promise error due to cancellation.\n        result.catch(error => {\n          if (!/^cancelled/iu.test(error.message)) {\n            throw error;\n          }\n        });\n      }\n\n      get onvoiceschanged() {\n        return getEventAttributeValue(this, 'voiceschanged');\n      }\n\n      set onvoiceschanged(value) {\n        setEventAttributeValue(this, 'voiceschanged', value);\n      }\n    }\n\n    return {\n      SpeechGrammarList,\n      SpeechRecognition,\n      speechSynthesis: new SpeechSynthesis(),\n      SpeechSynthesisUtterance: SpeechSynthesisAudioStreamUtterance\n    };\n  };\n}\n"]},"metadata":{},"sourceType":"script"}