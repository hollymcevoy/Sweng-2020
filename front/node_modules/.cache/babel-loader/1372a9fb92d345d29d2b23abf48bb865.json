{"ast":null,"code":"\"use strict\";\n\nvar _interopRequireDefault = require(\"@babel/runtime/helpers/interopRequireDefault\");\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.default = createCognitiveServicesSpeechServicesPonyfillFactory;\n\nvar _SpeechServices = _interopRequireDefault(require(\"web-speech-cognitive-services/lib/SpeechServices\"));\n\nvar _createMicrophoneAudioConfigAndAudioContext = _interopRequireDefault(require(\"./speech/createMicrophoneAudioConfigAndAudioContext\"));\n\nfunction createCognitiveServicesSpeechServicesPonyfillFactory(_ref) {\n  var audioConfig = _ref.audioConfig,\n      audioContext = _ref.audioContext,\n      audioInputDeviceId = _ref.audioInputDeviceId,\n      credentials = _ref.credentials,\n      enableTelemetry = _ref.enableTelemetry,\n      speechRecognitionEndpointId = _ref.speechRecognitionEndpointId,\n      speechSynthesisDeploymentId = _ref.speechSynthesisDeploymentId,\n      speechSynthesisOutputFormat = _ref.speechSynthesisOutputFormat,\n      textNormalization = _ref.textNormalization;\n\n  if (!window.navigator.mediaDevices && !audioConfig) {\n    console.warn('botframework-webchat: Your browser does not support Web Audio or the page is not loaded via HTTPS or localhost. Cognitive Services Speech Services is disabled. However, you may pass a custom AudioConfig to enable speech in this environment.');\n    return function () {\n      return {};\n    };\n  }\n\n  if (audioConfig) {\n    audioInputDeviceId && console.warn('botframework-webchat: \"audioConfig\" and \"audioInputDeviceId\" cannot be set at the same time; ignoring \"audioInputDeviceId\".');\n    audioContext && console.warn('botframework-webchat: \"audioConfig\" and \"audioContext\" cannot be set at the same time; ignoring \"audioContext\" for speech recognition.');\n  } else {\n    var _createMicrophoneAudi = (0, _createMicrophoneAudioConfigAndAudioContext.default)({\n      audioContext: audioContext,\n      audioInputDeviceId: audioInputDeviceId,\n      enableTelemetry: enableTelemetry\n    });\n\n    audioConfig = _createMicrophoneAudi.audioConfig;\n    audioContext = _createMicrophoneAudi.audioContext;\n  }\n\n  return function () {\n    var _ref2 = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {},\n        referenceGrammarID = _ref2.referenceGrammarID;\n\n    var _createPonyfill = (0, _SpeechServices.default)({\n      audioConfig: audioConfig,\n      audioContext: audioContext,\n      credentials: credentials,\n      enableTelemetry: enableTelemetry,\n      referenceGrammars: referenceGrammarID ? [\"luis/\".concat(referenceGrammarID, \"-PRODUCTION\")] : [],\n      speechRecognitionEndpointId: speechRecognitionEndpointId,\n      speechSynthesisDeploymentId: speechSynthesisDeploymentId,\n      speechSynthesisOutputFormat: speechSynthesisOutputFormat,\n      textNormalization: textNormalization\n    }),\n        SpeechGrammarList = _createPonyfill.SpeechGrammarList,\n        SpeechRecognition = _createPonyfill.SpeechRecognition,\n        speechSynthesis = _createPonyfill.speechSynthesis,\n        SpeechSynthesisUtterance = _createPonyfill.SpeechSynthesisUtterance;\n\n    return {\n      resumeAudioContext: function resumeAudioContext() {\n        return audioContext && audioContext.state === 'suspended' && audioContext.resume();\n      },\n      SpeechGrammarList: SpeechGrammarList,\n      SpeechRecognition: SpeechRecognition,\n      speechSynthesis: speechSynthesis,\n      SpeechSynthesisUtterance: SpeechSynthesisUtterance\n    };\n  };\n}","map":{"version":3,"mappings":";;;;;;;;;AAEA;;AAKA;;AAEe,SAASA,oDAAT,OAoBc;AAAA,MAnB3BC,WAmB2B,QAnB3BA,WAmB2B;AAAA,MAlB3BC,YAkB2B,QAlB3BA,YAkB2B;AAAA,MAjB3BC,kBAiB2B,QAjB3BA,kBAiB2B;AAAA,MAhB3BC,WAgB2B,QAhB3BA,WAgB2B;AAAA,MAf3BC,eAe2B,QAf3BA,eAe2B;AAAA,MAd3BC,2BAc2B,QAd3BA,2BAc2B;AAAA,MAb3BC,2BAa2B,QAb3BA,2BAa2B;AAAA,MAZ3BC,2BAY2B,QAZ3BA,2BAY2B;AAAA,MAX3BC,iBAW2B,QAX3BA,iBAW2B;;AAC3B,MAAI,CAACC,MAAM,CAACC,SAAPD,CAAiBE,YAAlB,IAAkC,CAACX,WAAvC,EAAoD;AAClDY,WAAO,CAACC,IAARD,CACE,kPADFA;AAIA,WAAO;AAAA,aAAO,EAAP;AAAP;AACD;;AAED,MAAIZ,WAAJ,EAAiB;AACfE,sBAAkB,IAChBU,OAAO,CAACC,IAARD,CACE,6HADFA,CADFV;AAKAD,gBAAY,IACVW,OAAO,CAACC,IAARD,CACE,wIADFA,CADFX;AANF,SAUO;AAAA,gCAC4B,yDAA2C;AAC1EA,kBAAY,EAAZA,YAD0E;AAE1EC,wBAAkB,EAAlBA,kBAF0E;AAG1EE,qBAAe,EAAfA;AAH0E,KAA3C,CAD5B;;AACFJ,eADE,yBACFA;AAAaC,gBADX,yBACWA;AAKjB;;AAED,SAAO,YAAiC;AAAA,oFAAP,EAAO;AAAA,QAA9Ba,kBAA8B,SAA9BA,kBAA8B;;AACtC,0BAA4F,6BAAe;AACzGd,iBAAW,EAAXA,WADyG;AAEzGC,kBAAY,EAAZA,YAFyG;AAGzGE,iBAAW,EAAXA,WAHyG;AAIzGC,qBAAe,EAAfA,eAJyG;AAKzGW,uBAAiB,EAAED,kBAAkB,GAAG,gBAASA,kBAAT,iBAAH,GAA+C,EALqB;AAMzGT,iCAA2B,EAA3BA,2BANyG;AAOzGC,iCAA2B,EAA3BA,2BAPyG;AAQzGC,iCAA2B,EAA3BA,2BARyG;AASzGC,uBAAiB,EAAjBA;AATyG,KAAf,CAA5F;AAAA,QAAQQ,iBAAR,mBAAQA,iBAAR;AAAA,QAA2BC,iBAA3B,mBAA2BA,iBAA3B;AAAA,QAA8CC,eAA9C,mBAA8CA,eAA9C;AAAA,QAA+DC,wBAA/D,mBAA+DA,wBAA/D;;AAYA,WAAO;AACLC,wBAAkB,EAAE;AAAA,eAAMnB,YAAY,IAAIA,YAAY,CAACoB,KAAbpB,KAAuB,WAAvCA,IAAsDA,YAAY,CAACqB,MAAbrB,EAA5D;AADf;AAELe,uBAAiB,EAAjBA,iBAFK;AAGLC,uBAAiB,EAAjBA,iBAHK;AAILC,qBAAe,EAAfA,eAJK;AAKLC,8BAAwB,EAAxBA;AALK,KAAP;AAbF;AAqBD","names":["createCognitiveServicesSpeechServicesPonyfillFactory","audioConfig","audioContext","audioInputDeviceId","credentials","enableTelemetry","speechRecognitionEndpointId","speechSynthesisDeploymentId","speechSynthesisOutputFormat","textNormalization","window","navigator","mediaDevices","console","warn","referenceGrammarID","referenceGrammars","SpeechGrammarList","SpeechRecognition","speechSynthesis","SpeechSynthesisUtterance","resumeAudioContext","state","resume"],"sources":["/Users/dylanmurray/Sweng-2022/front/node_modules/botframework-webchat/lib/src/createCognitiveServicesSpeechServicesPonyfillFactory.ts"],"sourcesContent":["import { AudioConfig } from 'microsoft-cognitiveservices-speech-sdk';\nimport { WebSpeechPonyfillFactory } from 'botframework-webchat-api';\nimport createPonyfill from 'web-speech-cognitive-services/lib/SpeechServices';\n\nimport CognitiveServicesAudioOutputFormat from './types/CognitiveServicesAudioOutputFormat';\nimport CognitiveServicesCredentials from './types/CognitiveServicesCredentials';\nimport CognitiveServicesTextNormalization from './types/CognitiveServicesTextNormalization';\nimport createMicrophoneAudioConfigAndAudioContext from './speech/createMicrophoneAudioConfigAndAudioContext';\n\nexport default function createCognitiveServicesSpeechServicesPonyfillFactory({\n  audioConfig,\n  audioContext,\n  audioInputDeviceId,\n  credentials,\n  enableTelemetry,\n  speechRecognitionEndpointId,\n  speechSynthesisDeploymentId,\n  speechSynthesisOutputFormat,\n  textNormalization\n}: {\n  audioConfig?: AudioConfig;\n  audioContext?: AudioContext;\n  audioInputDeviceId?: string;\n  credentials: CognitiveServicesCredentials;\n  enableTelemetry?: true;\n  speechRecognitionEndpointId?: string;\n  speechSynthesisDeploymentId?: string;\n  speechSynthesisOutputFormat?: CognitiveServicesAudioOutputFormat;\n  textNormalization?: CognitiveServicesTextNormalization;\n}): WebSpeechPonyfillFactory {\n  if (!window.navigator.mediaDevices && !audioConfig) {\n    console.warn(\n      'botframework-webchat: Your browser does not support Web Audio or the page is not loaded via HTTPS or localhost. Cognitive Services Speech Services is disabled. However, you may pass a custom AudioConfig to enable speech in this environment.'\n    );\n\n    return () => ({});\n  }\n\n  if (audioConfig) {\n    audioInputDeviceId &&\n      console.warn(\n        'botframework-webchat: \"audioConfig\" and \"audioInputDeviceId\" cannot be set at the same time; ignoring \"audioInputDeviceId\".'\n      );\n\n    audioContext &&\n      console.warn(\n        'botframework-webchat: \"audioConfig\" and \"audioContext\" cannot be set at the same time; ignoring \"audioContext\" for speech recognition.'\n      );\n  } else {\n    ({ audioConfig, audioContext } = createMicrophoneAudioConfigAndAudioContext({\n      audioContext,\n      audioInputDeviceId,\n      enableTelemetry\n    }));\n  }\n\n  return ({ referenceGrammarID } = {}) => {\n    const { SpeechGrammarList, SpeechRecognition, speechSynthesis, SpeechSynthesisUtterance } = createPonyfill({\n      audioConfig,\n      audioContext,\n      credentials,\n      enableTelemetry,\n      referenceGrammars: referenceGrammarID ? [`luis/${referenceGrammarID}-PRODUCTION`] : [],\n      speechRecognitionEndpointId,\n      speechSynthesisDeploymentId,\n      speechSynthesisOutputFormat,\n      textNormalization\n    });\n\n    return {\n      resumeAudioContext: () => audioContext && audioContext.state === 'suspended' && audioContext.resume(),\n      SpeechGrammarList,\n      SpeechRecognition,\n      speechSynthesis,\n      SpeechSynthesisUtterance\n    };\n  };\n}\n"]},"metadata":{},"sourceType":"script"}